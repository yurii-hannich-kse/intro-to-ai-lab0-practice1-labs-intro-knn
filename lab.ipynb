{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 0: Introduction to ML Workflow and K-Nearest Neighbors\n",
    "\n",
    "**Course:** Introduction to AI (Spring 2025/2026)  \n",
    "**Instructor:** Yurii Hannich  \n",
    "**Points:** 6  \n",
    "**Completed:** During P1 session\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Goals\n",
    "\n",
    "This lab introduces you to:\n",
    "- Working with Jupyter Notebooks\n",
    "- GitHub Classroom workflow\n",
    "- Basic ML pipeline: load data â†’ train â†’ predict â†’ evaluate\n",
    "- Your first ML algorithm: K-Nearest Neighbors\n",
    "\n",
    "**Note:** This is a guided practice session. We'll implement simple functions together, and they will be automatically tested.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Loading"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "! pip install numpy matplotlib scikit-learn",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Import libraries\n",
    "import numpy as np  # For numerical operations and arrays\n",
    "import matplotlib.pyplot as plt  # For creating visualizations\n",
    "from sklearn.datasets import load_iris  # Built-in dataset loader\n",
    "from sklearn.model_selection import train_test_split  # For splitting data into train/test sets\n",
    "from sklearn.neighbors import KNeighborsClassifier  # Professional KNN implementation\n",
    "from sklearn.metrics import accuracy_score  # For evaluating model performance\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features: measurements of flowers\n",
    "y = iris.target  # Labels: species (0, 1, or 2)\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Feature names: {iris.feature_names}\")\n",
    "print(f\"Target names: {iris.target_names}\")\n",
    "print(f\"\\nFirst 3 samples:\\n{X[:3]}\")\n",
    "print(f\"First 3 labels: {y[:3]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤” Discussion Questions\n",
    "\n",
    "Before we continue, let's think about what we're dealing with:\n",
    "\n",
    "1. **What is the dimensionality of the feature space?**  \n",
    "   _(Hint: How many features does each sample have?)_\n",
    "\n",
    "2. **Is this a classification or regression task?**  \n",
    "   _(Hint: Look at the target values - are they continuous numbers or discrete categories?)_\n",
    "\n",
    "3. **How many classes do we need to predict?**\n",
    "   _(Hint: Look at the unique values in the target array)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Visualizing the Data\n",
    "\n",
    "Let's see what our data looks like. We'll plot two features at a time."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize petal dimensions (features 2 and 3)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot each species with different colors\n",
    "for i, species in enumerate(iris.target_names):\n",
    "    mask = y == i\n",
    "    plt.scatter(X[mask, 2], X[mask, 3], label=species, alpha=0.7, s=100)\n",
    "\n",
    "plt.xlabel('Petal Length (cm)', fontsize=12)\n",
    "plt.ylabel('Petal Width (cm)', fontsize=12)\n",
    "plt.title('Iris Dataset: Petal Dimensions', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how the species form clusters? That's why KNN works!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Train-Test Split\n",
    "\n",
    "We split data into:\n",
    "- **Training set (70%)**: Used to \"teach\" the algorithm\n",
    "- **Test set (30%)**: Used to evaluate on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Implementing KNN from Scratch\n",
    "\n",
    "### How KNN Works:\n",
    "\n",
    "1. Calculate distance from new point to all training points\n",
    "2. Find K nearest neighbors\n",
    "3. Take majority vote among neighbors\n",
    "4. Return the most common class\n",
    "\n",
    "We'll implement this step by step with **testable functions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Euclidean Distance\n",
    "\n",
    "First, we need a way to measure how \"close\" two points are.  \n",
    "Formula: \\\\( d(p, q) = \\sqrt{\\sum_{i=1}^{n} (p_i - q_i)^2} \\\\)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def euclidean_distance(point1, point2):\n",
    "    \"\"\"\n",
    "    Calculate Euclidean distance between two points.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    point1 : numpy array\n",
    "        First point\n",
    "    point2 : numpy array\n",
    "        Second point\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Distance between the points\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> euclidean_distance(np.array([0, 0]), np.array([3, 4]))\n",
    "    5.0\n",
    "    \"\"\"\n",
    "    # TODO: Implement the Euclidean distance formula\n",
    "    # Hint: Use np.sqrt() and np.sum()\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test your function\n",
    "p1 = np.array([0, 0])\n",
    "p2 = np.array([3, 4])\n",
    "dist = euclidean_distance(p1, p2)\n",
    "print(f\"Distance between {p1} and {p2}: {dist}\")\n",
    "print(f\"Expected: 5.0\")\n",
    "\n",
    "# Another test\n",
    "p3 = np.array([1, 2, 3])\n",
    "p4 = np.array([4, 5, 6])\n",
    "dist2 = euclidean_distance(p3, p4)\n",
    "print(f\"\\nDistance between {p3} and {p4}: {dist2:.2f}\")\n",
    "print(f\"Expected: 5.20\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: Classify a Single Example\n",
    "\n",
    "Now let's classify ONE test point using the KNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def classify_example(X_train, y_train, test_point, k=3):\n",
    "    \"\"\"\n",
    "    Classify a single test point using KNN.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : numpy array, shape (n_samples, n_features)\n",
    "        Training data\n",
    "    y_train : numpy array, shape (n_samples,)\n",
    "        Training labels\n",
    "    test_point : numpy array, shape (n_features,)\n",
    "        Point to classify\n",
    "    k : int\n",
    "        Number of neighbors to consider\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    int\n",
    "        Predicted class label\n",
    "    \"\"\"\n",
    "    # TODO: Implement KNN classification for one point\n",
    "    # Steps:\n",
    "    # 1. Calculate distances from test_point to all training points\n",
    "    # 2. Find indices of k smallest distances (use np.argsort)\n",
    "    # 3. Get labels of k nearest neighbors\n",
    "    # 4. Return most common label (use np.bincount and np.argmax)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test your function\n",
    "test_sample = X_test[0]\n",
    "prediction = classify_example(X_train, y_train, test_sample, k=3)\n",
    "actual = y_test[0]\n",
    "\n",
    "print(f\"Test sample features: {test_sample}\")\n",
    "print(f\"Predicted class: {prediction} ({iris.target_names[prediction]})\")\n",
    "print(f\"Actual class: {actual} ({iris.target_names[actual]})\")\n",
    "print(f\"Correct: {prediction == actual}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.3: Predict Multiple Examples\n",
    "\n",
    "Now let's classify ALL test points at once."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def predict(X_train, y_train, X_test, k=3):\n",
    "    \"\"\"\n",
    "    Classify multiple test points using KNN.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : numpy array, shape (n_samples, n_features)\n",
    "        Training data\n",
    "    y_train : numpy array, shape (n_samples,)\n",
    "        Training labels\n",
    "    X_test : numpy array, shape (n_test_samples, n_features)\n",
    "        Test data to classify\n",
    "    k : int\n",
    "        Number of neighbors to consider\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy array, shape (n_test_samples,)\n",
    "        Predicted class labels\n",
    "    \"\"\"\n",
    "    # TODO: Use classify_example for each test point\n",
    "    # Hint: Loop through X_test and collect predictions\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test your predict function\n",
    "y_pred_scratch = predict(X_train, y_train, X_test, k=3)\n",
    "accuracy_scratch = accuracy_score(y_test, y_pred_scratch)\n",
    "\n",
    "print(f\"Predictions: {y_pred_scratch}\")\n",
    "print(f\"Actual labels: {y_test}\")\n",
    "print(f\"\\nAccuracy: {accuracy_scratch:.2%}\")\n",
    "print(f\"Correct predictions: {np.sum(y_pred_scratch == y_test)}/{len(y_test)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Using Scikit-Learn\n",
    "\n",
    "Great! Now you understand how KNN works internally.  \n",
    "In practice, we use optimized libraries like scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create and train KNN using sklearn\n",
    "knn_sklearn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_sklearn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_sklearn = knn_sklearn.predict(X_test)\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "\n",
    "print(f\"Scikit-learn accuracy: {accuracy_sklearn:.2%}\")\n",
    "print(f\"\\nOur implementation: {accuracy_scratch:.2%}\")\n",
    "print(f\"Sklearn implementation: {accuracy_sklearn:.2%}\")\n",
    "print(f\"\\nâœ“ Both implementations should give the same results!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Visualizing Decision Boundaries\n",
    "\n",
    "Let's see how KNN makes decisions visually. We'll use only 2 features for easy visualization."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Use only petal length and petal width for visualization\n",
    "X_2d = X[:, 2:4]  # Features 2 and 3\n",
    "X_train_2d, X_test_2d, y_train_2d, y_test_2d = train_test_split(\n",
    "    X_2d, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train KNN on 2D data\n",
    "knn_2d = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_2d.fit(X_train_2d, y_train_2d)\n",
    "\n",
    "# Create a mesh to plot decision boundaries\n",
    "h = 0.02  # Step size in the mesh\n",
    "x_min, x_max = X_2d[:, 0].min() - 0.5, X_2d[:, 0].max() + 0.5\n",
    "y_min, y_max = X_2d[:, 1].min() - 0.5, X_2d[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Predict for each point in the mesh\n",
    "Z = knn_2d.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Left plot: Decision boundaries\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "for i, species in enumerate(iris.target_names):\n",
    "    mask = y == i\n",
    "    plt.scatter(X_2d[mask, 0], X_2d[mask, 1], label=species, alpha=0.7, s=50, edgecolor='black')\n",
    "plt.xlabel('Petal Length (cm)')\n",
    "plt.ylabel('Petal Width (cm)')\n",
    "plt.title('KNN Decision Boundaries (k=3)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Right plot: Test points\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "# Show training points in gray\n",
    "plt.scatter(X_train_2d[:, 0], X_train_2d[:, 1], c='gray', alpha=0.3, s=30, label='Training')\n",
    "# Show test points with actual colors\n",
    "y_pred_2d = knn_2d.predict(X_test_2d)\n",
    "correct = y_pred_2d == y_test_2d\n",
    "plt.scatter(X_test_2d[correct, 0], X_test_2d[correct, 1], c='green', s=100, \n",
    "           marker='o', edgecolor='black', linewidth=2, label='Correct')\n",
    "plt.scatter(X_test_2d[~correct, 0], X_test_2d[~correct, 1], c='red', s=100, \n",
    "           marker='x', linewidth=3, label='Wrong')\n",
    "plt.xlabel('Petal Length (cm)')\n",
    "plt.ylabel('Petal Width (cm)')\n",
    "plt.title(f'Test Predictions (Accuracy: {accuracy_score(y_test_2d, y_pred_2d):.2%})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤” Discussion: Decision Boundaries\n",
    "\n",
    "Look at the visualization above:\n",
    "\n",
    "1. **What do the colored regions represent?**  \n",
    "   _(Hint: These are the decision boundaries - where KNN would classify any point in that region)_\n",
    "\n",
    "2. **Why are the boundaries not straight lines?**  \n",
    "   _(Think about how KNN makes decisions - it's based on nearest neighbors, not a linear equation)_\n",
    "\n",
    "3. **What happens if we change k to 1 or 10?**  \n",
    "   _(You can experiment by changing n_neighbors above and re-running!)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Experimenting with K\n",
    "\n",
    "Let's quickly see how different K values affect our results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Try different k values\n",
    "k_values = [1, 3, 5, 7, 10, 15]\n",
    "print(\"K value â†’ Accuracy\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"k = {k:2d}  â†’  {acc:.2%}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've completed Lab 0! You now know:\n",
    "\n",
    "âœ… How to work with Jupyter Notebooks  \n",
    "âœ… How to load and explore data  \n",
    "âœ… How KNN algorithm works internally  \n",
    "âœ… How to implement ML algorithms from scratch  \n",
    "âœ… How to use scikit-learn for faster implementation  \n",
    "âœ… How to visualize decision boundaries  \n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **KNN is a \"lazy learner\"**: It doesn't really train, just stores data\n",
    "- **Distance matters**: Similar points should be close in feature space\n",
    "- **K is important**: Too small â†’ overfitting, too large â†’ underfitting\n",
    "- **Libraries exist for a reason**: Sklearn is faster and more optimized!\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Make sure all cells run without errors\n",
    "2. Commit and push to GitHub\n",
    "3. Your code will be automatically tested\n",
    "4. In Lab 1, you'll work more independently!\n",
    "\n",
    "---\n",
    "\n",
    "**Remember the AI usage policy:** Use approved prompts only! ðŸ¤–"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
